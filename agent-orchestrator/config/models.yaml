models:
  # === OpenAI Models ===
  - name: gpt-4-turbo
    provider: openai
    cost_level: high
    latency: medium
    max_tokens: 128000
    available: true
    # Pricing (credits per 1K tokens and USD cost reference)
    pricing:
      credits_per_1k_input: 25.0
      credits_per_1k_output: 50.0
      usd_per_1k_input: 0.01
      usd_per_1k_output: 0.03
    capabilities:
      reasoning: 9
      coding: 9
      long_context: 9
      summarization: 8
      planning: 8
      structured_output: 8
      
  - name: gpt-4o
    provider: openai
    cost_level: high
    latency: fast
    max_tokens: 128000
    available: true
    pricing:
      credits_per_1k_input: 20.0
      credits_per_1k_output: 40.0
      usd_per_1k_input: 0.005
      usd_per_1k_output: 0.015
    capabilities:
      reasoning: 9
      coding: 9
      long_context: 9
      summarization: 9
      planning: 8
      multimodal: 9
      structured_output: 9

  - name: gpt-3.5-turbo
    provider: openai
    cost_level: low
    latency: fast
    max_tokens: 16000
    available: true
    pricing:
      credits_per_1k_input: 3.0
      credits_per_1k_output: 6.0
      usd_per_1k_input: 0.0005
      usd_per_1k_output: 0.0015
    capabilities:
      reasoning: 6
      coding: 6
      summarization: 7
      speed: 9
      structured_output: 7

  # === Anthropic Models ===
  - name: claude-3-5-sonnet-20241022
    provider: anthropic
    cost_level: high
    latency: medium
    max_tokens: 200000
    available: true
    pricing:
      credits_per_1k_input: 15.0
      credits_per_1k_output: 75.0
      usd_per_1k_input: 0.003
      usd_per_1k_output: 0.015
    capabilities:
      reasoning: 10
      coding: 10
      long_context: 10
      summarization: 9
      planning: 9
      structured_output: 9

  - name: claude-3-haiku-20240307
    provider: anthropic
    cost_level: low
    latency: fast
    max_tokens: 200000
    available: true
    pricing:
      credits_per_1k_input: 1.0
      credits_per_1k_output: 5.0
      usd_per_1k_input: 0.00025
      usd_per_1k_output: 0.00125
    capabilities:
      reasoning: 7
      coding: 7
      summarization: 8
      speed: 9
      structured_output: 8

  # === Groq Models (Fast Inference) ===
  - name: llama-3.3-70b-versatile
    provider: groq
    cost_level: low
    latency: fast
    max_tokens: 32000
    available: true
    pricing:
      credits_per_1k_input: 0.5
      credits_per_1k_output: 1.0
      usd_per_1k_input: 0.00006
      usd_per_1k_output: 0.00024
    capabilities:
      reasoning: 8
      coding: 8
      summarization: 8
      speed: 10
      structured_output: 7

  - name: mixtral-8x7b-32768
    provider: groq
    cost_level: low
    latency: fast
    max_tokens: 32768
    available: true
    pricing:
      credits_per_1k_input: 0.3
      credits_per_1k_output: 0.6
      usd_per_1k_input: 0.00003
      usd_per_1k_output: 0.00006
    capabilities:
      reasoning: 7
      coding: 7
      summarization: 7
      speed: 10

  # === Ollama Local Models (Free) ===
  - name: llama3:8b
    provider: ollama
    cost_level: free
    latency: medium
    max_tokens: 8000
    available: true
    pricing:
      credits_per_1k_input: 0.0
      credits_per_1k_output: 0.0
      usd_per_1k_input: 0.0
      usd_per_1k_output: 0.0
    capabilities:
      reasoning: 6
      coding: 6
      summarization: 6
      speed: 8

  - name: llama3:70b
    provider: ollama
    cost_level: free
    latency: slow
    max_tokens: 8000
    available: true
    pricing:
      credits_per_1k_input: 0.0
      credits_per_1k_output: 0.0
      usd_per_1k_input: 0.0
      usd_per_1k_output: 0.0
    capabilities:
      reasoning: 8
      coding: 8
      summarization: 8
      speed: 4

  - name: codellama:13b
    provider: ollama
    cost_level: free
    latency: medium
    max_tokens: 16000
    available: true
    pricing:
      credits_per_1k_input: 0.0
      credits_per_1k_output: 0.0
      usd_per_1k_input: 0.0
      usd_per_1k_output: 0.0
    capabilities:
      reasoning: 5
      coding: 8
      summarization: 5
      speed: 7

  - name: mistral:7b
    provider: ollama
    cost_level: free
    latency: fast
    max_tokens: 8000
    available: true
    pricing:
      credits_per_1k_input: 0.0
      credits_per_1k_output: 0.0
      usd_per_1k_input: 0.0
      usd_per_1k_output: 0.0
    capabilities:
      reasoning: 6
      coding: 6
      summarization: 7
      speed: 9

# Default model fallback order by provider
defaults:
  openai: gpt-4-turbo
  anthropic: claude-3-5-sonnet-20241022
  groq: llama-3.3-70b-versatile
  ollama: llama3:8b

